{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "boston housing price prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZ_mq_L1fhXH",
        "colab_type": "text"
      },
      "source": [
        "## Regressor for boston housing dataset using pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_iQ3n92fw6O",
        "colab_type": "text"
      },
      "source": [
        "1.loading boston housing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zppwuf6e7v7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# load the boston housing data and split it in to train and target data\n",
        "boston = load_boston()\n",
        "x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4gA_ulqMfHI",
        "colab_type": "text"
      },
      "source": [
        "normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB517XSiNxsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = x_train.mean(axis=0)\n",
        "std = x_train.std(axis=0)\n",
        "\n",
        "# normalization on train data\n",
        "x_train_scaled = x_train - mean\n",
        "x_train_scaled =x_train_scaled/std\n",
        "# normalization on test data\n",
        "x_test_scaled = x_test - mean\n",
        "x_test_scaled /= std\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDqs-8rhNw-I",
        "colab_type": "text"
      },
      "source": [
        "min max normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kue-EeMYNwg3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# min-max normaization\n",
        "\n",
        "# compute the min and max value per feature on the training set\n",
        "min_on_training = x_train.min(axis=0)\n",
        "max_on_training = x_train.max(axis=0)\n",
        "\n",
        "# prevent divison by zero\n",
        "norm_range = max_on_training - min_on_training\n",
        "norm_range[norm_range == 0] = 1\n",
        "\n",
        "# subtract the min, and divide by range\n",
        "x_train_scaled = (x_train - min_on_training) / norm_range\n",
        "x_test_scaled = (x_test - min_on_training) / norm_range\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lTgBfwHVucz",
        "colab_type": "text"
      },
      "source": [
        "2.defining model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9BS97VQUL6t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "class Regressor(torch.nn.Module):\n",
        "    def __init__(self, inputSize, outputSize):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.mlp=nn.Sequential(\n",
        "            nn.Linear(inputSize,17),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(17,outputSize)\n",
        "        )\n",
        "        #self.linear = torch.nn.Linear(inputSize, outputSize)\n",
        "        #self.Relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_ = x.view(x.size(0),-1)\n",
        "        out = self.mlp(x_)\n",
        "        #x = self.Relu(x)\n",
        "        #out = self.linear(x)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcHv71nPUkv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputDim = 13        # takes variable 'x' \n",
        "outputDim = 1       # takes variable 'y'\n",
        "learningRate = 0.01 \n",
        "epochs = 4000\n",
        "\n",
        "model = Regressor(inputDim, outputDim)\n",
        "##### For GPU #######\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loyaZul_U0Tj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = torch.nn.MSELoss() \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEMe-gYZ3RcH",
        "colab_type": "text"
      },
      "source": [
        "3. training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_pGnmDHU5Cv",
        "colab_type": "code",
        "outputId": "6f6c1a74-8ac5-4c31-f9ad-493c3ac2561c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "loss_item = []\n",
        "loss_test = []\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    test_input = Variable(torch.from_numpy(x_test_scaled).cuda())\n",
        "    test_label = Variable(torch.from_numpy(y_test).cuda())\n",
        "else:\n",
        "    test_input = Variable(torch.from_numpy(x_test_scaled))\n",
        "    test_label = Variable(torch.from_numpy(y_test))\n",
        "y_pred = model(test_input.float())\n",
        "test_loss = criterion(y_pred, test_label.float())\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Converting inputs and labels to Variable\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = Variable(torch.from_numpy(x_train_scaled).cuda())\n",
        "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
        "    else:\n",
        "        inputs = Variable(torch.from_numpy(x_train_scaled))\n",
        "        labels = Variable(torch.from_numpy(y_train))\n",
        "\n",
        "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # get output from the model, given the inputs\n",
        "    outputs = model(inputs.float())\n",
        "    test_output = model(test_input.float())\n",
        "\n",
        "    #resize the label\n",
        "    labels = labels.view(labels.size(0),-1)\n",
        "    test_label = test_label.view(test_label.size(0),-1)\n",
        "\n",
        "\n",
        "    # get loss for the predicted output\n",
        "    loss = criterion(outputs, labels.float())\n",
        "    loss1=criterion(test_output,test_label.float())\n",
        "\n",
        "    loss_item.append(loss)\n",
        "    loss_test.append(loss1)\n",
        "    # get gradients w.r.t to parameters\n",
        "    loss.backward()\n",
        "    \n",
        "    # update parameters\n",
        "    optimizer.step()\n",
        "    if epoch%10==0:\n",
        "      print('epoch {}, loss {}, train r2 {}, test_r2 {}'.format(epoch, loss.item(),r2_score(y_train,outputs.detach().numpy()),r2_score(y_test,test_output.detach().numpy())))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([127])) that is different to the input size (torch.Size([127, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0, loss 597.5408325195312, train r2 -6.0044916831774815, test_r2 -6.111472493643386\n",
            "epoch 10, loss 546.7642211914062, train r2 -5.409278556667833, test_r2 -5.524869230016055\n",
            "epoch 20, loss 456.876220703125, train r2 -4.3555932409419995, test_r2 -4.476917328968409\n",
            "epoch 30, loss 326.55078125, train r2 -2.8278926971867344, test_r2 -2.986405838180806\n",
            "epoch 40, loss 189.80633544921875, train r2 -1.2249474439782766, test_r2 -1.4452010603756906\n",
            "epoch 50, loss 105.34386444091797, train r2 -0.2348616032029378, test_r2 -0.49328434087359896\n",
            "epoch 60, loss 72.69058227539062, train r2 0.147906581230271, test_r2 -0.10829308563616702\n",
            "epoch 70, loss 47.709537506103516, train r2 0.44073938180267536, test_r2 0.18729711095547785\n",
            "epoch 80, loss 32.055599212646484, train r2 0.6242379283317833, test_r2 0.378076498038303\n",
            "epoch 90, loss 25.58208465576172, train r2 0.7001217526088221, test_r2 0.4710428114014197\n",
            "epoch 100, loss 22.718727111816406, train r2 0.733686594374305, test_r2 0.5233399083892478\n",
            "epoch 110, loss 20.858596801757812, train r2 0.7554914109719203, test_r2 0.5616235746733997\n",
            "epoch 120, loss 19.260141372680664, train r2 0.7742288218048832, test_r2 0.5897886376470132\n",
            "epoch 130, loss 18.013141632080078, train r2 0.7888463813121097, test_r2 0.6078844277089702\n",
            "epoch 140, loss 16.98984146118164, train r2 0.8008417089458301, test_r2 0.6221215478456272\n",
            "epoch 150, loss 16.12278938293457, train r2 0.8110054669411698, test_r2 0.6353120334845936\n",
            "epoch 160, loss 15.362039566040039, train r2 0.8199231326117873, test_r2 0.645667056454403\n",
            "epoch 170, loss 14.707138061523438, train r2 0.8276000171479343, test_r2 0.6547615047493225\n",
            "epoch 180, loss 14.130496978759766, train r2 0.8343595158594809, test_r2 0.6633626423155495\n",
            "epoch 190, loss 13.60266399383545, train r2 0.8405468777211317, test_r2 0.6718765650747158\n",
            "epoch 200, loss 13.125618934631348, train r2 0.8461389021376893, test_r2 0.6792158171406641\n",
            "epoch 210, loss 12.698084831237793, train r2 0.8511505352890643, test_r2 0.6870134695138008\n",
            "epoch 220, loss 12.30119800567627, train r2 0.8558029304929403, test_r2 0.6933820693931148\n",
            "epoch 230, loss 11.9273042678833, train r2 0.8601857841138009, test_r2 0.6994177497278499\n",
            "epoch 240, loss 11.56375503540039, train r2 0.8644473762264716, test_r2 0.7049602904831818\n",
            "epoch 250, loss 11.209206581115723, train r2 0.8686034665237938, test_r2 0.709522952833676\n",
            "epoch 260, loss 10.876635551452637, train r2 0.8725019314998651, test_r2 0.7143035350526681\n",
            "epoch 270, loss 10.565531730651855, train r2 0.8761487518051824, test_r2 0.7189439838708702\n",
            "epoch 280, loss 10.272767066955566, train r2 0.8795805914471103, test_r2 0.7226054190909579\n",
            "epoch 290, loss 10.010953903198242, train r2 0.8826496168909514, test_r2 0.7271627427421796\n",
            "epoch 300, loss 9.772676467895508, train r2 0.8854427591807044, test_r2 0.7308591695225722\n",
            "epoch 310, loss 9.555928230285645, train r2 0.8879835148614826, test_r2 0.7347791604333499\n",
            "epoch 320, loss 9.354836463928223, train r2 0.8903407534409242, test_r2 0.7379462683453534\n",
            "epoch 330, loss 9.165446281433105, train r2 0.8925608221029455, test_r2 0.7412621455894831\n",
            "epoch 340, loss 8.990955352783203, train r2 0.8946062443486216, test_r2 0.7441643586863453\n",
            "epoch 350, loss 8.838236808776855, train r2 0.8963964462001306, test_r2 0.7469886221851957\n",
            "epoch 360, loss 8.698701858520508, train r2 0.8980320920844239, test_r2 0.7493439002696871\n",
            "epoch 370, loss 8.56623363494873, train r2 0.8995849155061707, test_r2 0.7514661975043454\n",
            "epoch 380, loss 8.444925308227539, train r2 0.9010069190978609, test_r2 0.7539342783382811\n",
            "epoch 390, loss 8.33320140838623, train r2 0.9023165626026255, test_r2 0.7561737697974307\n",
            "epoch 400, loss 8.22696304321289, train r2 0.9035619149996365, test_r2 0.7577744673563742\n",
            "epoch 410, loss 8.131040573120117, train r2 0.9046863303114904, test_r2 0.7591189141225689\n",
            "epoch 420, loss 8.040459632873535, train r2 0.9057481441906228, test_r2 0.7605677668275744\n",
            "epoch 430, loss 7.951228618621826, train r2 0.90679412724794, test_r2 0.7615806867636411\n",
            "epoch 440, loss 7.869524955749512, train r2 0.9077518718643622, test_r2 0.7628040854426551\n",
            "epoch 450, loss 7.791316032409668, train r2 0.9086686548128151, test_r2 0.7635935906627722\n",
            "epoch 460, loss 7.716161251068115, train r2 0.9095496318130911, test_r2 0.7648264581758206\n",
            "epoch 470, loss 7.642724990844727, train r2 0.9104104663677106, test_r2 0.765696736966307\n",
            "epoch 480, loss 7.567373752593994, train r2 0.9112937481303262, test_r2 0.7666239997017816\n",
            "epoch 490, loss 7.4935197830200195, train r2 0.9121594828517046, test_r2 0.7675225439470051\n",
            "epoch 500, loss 7.422114849090576, train r2 0.9129965034799405, test_r2 0.7679824054249021\n",
            "epoch 510, loss 7.355185508728027, train r2 0.9137810643904191, test_r2 0.7685502268062292\n",
            "epoch 520, loss 7.289212703704834, train r2 0.9145544078962896, test_r2 0.7690022579627552\n",
            "epoch 530, loss 7.228191375732422, train r2 0.9152697116042889, test_r2 0.7697347926164486\n",
            "epoch 540, loss 7.168844223022461, train r2 0.9159653899517287, test_r2 0.7699885973012264\n",
            "epoch 550, loss 7.107949256896973, train r2 0.91667921524486, test_r2 0.7701115382726309\n",
            "epoch 560, loss 7.051599025726318, train r2 0.9173397638715451, test_r2 0.7702213074098815\n",
            "epoch 570, loss 6.998531818389893, train r2 0.9179618269555782, test_r2 0.770678391896481\n",
            "epoch 580, loss 6.9486188888549805, train r2 0.918546917926383, test_r2 0.7710058786107207\n",
            "epoch 590, loss 6.9002275466918945, train r2 0.9191141688835416, test_r2 0.7713440630070645\n",
            "epoch 600, loss 6.853129863739014, train r2 0.9196662584307376, test_r2 0.7715771010624687\n",
            "epoch 610, loss 6.80802059173584, train r2 0.9201950345972356, test_r2 0.7723374636720075\n",
            "epoch 620, loss 6.763889789581299, train r2 0.9207123485929064, test_r2 0.7727909412838221\n",
            "epoch 630, loss 6.72056770324707, train r2 0.9212201749858779, test_r2 0.7733540170869279\n",
            "epoch 640, loss 6.680307865142822, train r2 0.9216921087139511, test_r2 0.7738045741882436\n",
            "epoch 650, loss 6.629871368408203, train r2 0.922283337778397, test_r2 0.7739206348748264\n",
            "epoch 660, loss 6.571477890014648, train r2 0.9229678354711094, test_r2 0.7742140768268855\n",
            "epoch 670, loss 6.512030124664307, train r2 0.9236646977366751, test_r2 0.7741287126312635\n",
            "epoch 680, loss 6.457760334014893, train r2 0.9243008581938401, test_r2 0.774238741838049\n",
            "epoch 690, loss 6.409907817840576, train r2 0.9248617910089676, test_r2 0.7744961805392737\n",
            "epoch 700, loss 6.367250919342041, train r2 0.9253618253577589, test_r2 0.7751070924505254\n",
            "epoch 710, loss 6.3267998695373535, train r2 0.9258360000169253, test_r2 0.7753464940352577\n",
            "epoch 720, loss 6.288741588592529, train r2 0.9262821269683359, test_r2 0.7756494115919312\n",
            "epoch 730, loss 6.252691268920898, train r2 0.926704719564594, test_r2 0.7761309329021583\n",
            "epoch 740, loss 6.218113899230957, train r2 0.9271100406352628, test_r2 0.776703885200274\n",
            "epoch 750, loss 6.186209201812744, train r2 0.9274840328841637, test_r2 0.7771835162476168\n",
            "epoch 760, loss 6.156445503234863, train r2 0.9278329292473619, test_r2 0.777652907973006\n",
            "epoch 770, loss 6.12606954574585, train r2 0.9281890046667155, test_r2 0.7778392367803748\n",
            "epoch 780, loss 6.097695827484131, train r2 0.9285216027499953, test_r2 0.7780376715567818\n",
            "epoch 790, loss 6.07125186920166, train r2 0.9288315860185973, test_r2 0.778050842089078\n",
            "epoch 800, loss 6.043580055236816, train r2 0.9291559620009436, test_r2 0.7783409444104996\n",
            "epoch 810, loss 6.01619291305542, train r2 0.929476995520152, test_r2 0.7787187068898441\n",
            "epoch 820, loss 5.990353107452393, train r2 0.9297798964833844, test_r2 0.7789299834262938\n",
            "epoch 830, loss 5.966547966003418, train r2 0.9300589473364773, test_r2 0.7790530555108107\n",
            "epoch 840, loss 5.9423346519470215, train r2 0.9303427787338445, test_r2 0.778957074991577\n",
            "epoch 850, loss 5.91831636428833, train r2 0.9306243266086544, test_r2 0.7794484093185352\n",
            "epoch 860, loss 5.894908428192139, train r2 0.9308987173000255, test_r2 0.7797244138028924\n",
            "epoch 870, loss 5.869568347930908, train r2 0.9311957620844665, test_r2 0.7803526413810088\n",
            "epoch 880, loss 5.843596458435059, train r2 0.9315002063241407, test_r2 0.780503576628435\n",
            "epoch 890, loss 5.819674968719482, train r2 0.9317806186011414, test_r2 0.7811740538388947\n",
            "epoch 900, loss 5.794151306152344, train r2 0.9320798143114246, test_r2 0.7811424009352925\n",
            "epoch 910, loss 5.767457485198975, train r2 0.9323927225634422, test_r2 0.7812930903640682\n",
            "epoch 920, loss 5.739881992340088, train r2 0.9327159691643309, test_r2 0.7817922085791558\n",
            "epoch 930, loss 5.71308708190918, train r2 0.9330300646444905, test_r2 0.7824048210672458\n",
            "epoch 940, loss 5.686176300048828, train r2 0.9333455162683243, test_r2 0.7826944035414526\n",
            "epoch 950, loss 5.660073280334473, train r2 0.9336515034527455, test_r2 0.7832084579409473\n",
            "epoch 960, loss 5.636603832244873, train r2 0.9339266184353326, test_r2 0.7830844803814057\n",
            "epoch 970, loss 5.613621711730957, train r2 0.934196016418231, test_r2 0.7839186491632864\n",
            "epoch 980, loss 5.587911605834961, train r2 0.934497398391082, test_r2 0.7840169115726027\n",
            "epoch 990, loss 5.566261291503906, train r2 0.9347511865063207, test_r2 0.7838977421056589\n",
            "epoch 1000, loss 5.544775009155273, train r2 0.9350030535024944, test_r2 0.7843686947373276\n",
            "epoch 1010, loss 5.523117542266846, train r2 0.9352569258756718, test_r2 0.7844761120484829\n",
            "epoch 1020, loss 5.502568244934082, train r2 0.9354978063280933, test_r2 0.7845667383263624\n",
            "epoch 1030, loss 5.482141017913818, train r2 0.9357372595512796, test_r2 0.784622623890334\n",
            "epoch 1040, loss 5.460847854614258, train r2 0.9359868601131258, test_r2 0.7849296161295949\n",
            "epoch 1050, loss 5.440202713012695, train r2 0.9362288678858339, test_r2 0.7852451864615172\n",
            "epoch 1060, loss 5.421285152435303, train r2 0.9364506223024246, test_r2 0.7857400906841083\n",
            "epoch 1070, loss 5.403430938720703, train r2 0.9366599144259232, test_r2 0.7859356044840388\n",
            "epoch 1080, loss 5.386291027069092, train r2 0.9368608335034154, test_r2 0.7862461341595673\n",
            "epoch 1090, loss 5.370162010192871, train r2 0.9370498982828991, test_r2 0.7863542135928375\n",
            "epoch 1100, loss 5.354058742523193, train r2 0.9372386660743048, test_r2 0.7865075168784039\n",
            "epoch 1110, loss 5.338919162750244, train r2 0.9374161328702143, test_r2 0.7867537084044505\n",
            "epoch 1120, loss 5.324739933013916, train r2 0.9375823465472217, test_r2 0.7869848436467628\n",
            "epoch 1130, loss 5.3105268478393555, train r2 0.9377489550095117, test_r2 0.787184940695823\n",
            "epoch 1140, loss 5.297155380249023, train r2 0.9379056999521336, test_r2 0.7876059906695997\n",
            "epoch 1150, loss 5.283538341522217, train r2 0.9380653212911191, test_r2 0.7878075283157969\n",
            "epoch 1160, loss 5.271041393280029, train r2 0.9382118086839505, test_r2 0.7881510890436512\n",
            "epoch 1170, loss 5.2574543952941895, train r2 0.9383710819329099, test_r2 0.788293290911301\n",
            "epoch 1180, loss 5.243381023406982, train r2 0.9385360487973554, test_r2 0.7883312708367705\n",
            "epoch 1190, loss 5.2306742668151855, train r2 0.9386850007775576, test_r2 0.7885255934960871\n",
            "epoch 1200, loss 5.218245506286621, train r2 0.9388306934189452, test_r2 0.7886464761520262\n",
            "epoch 1210, loss 5.205957412719727, train r2 0.9389747367353236, test_r2 0.7886765732163875\n",
            "epoch 1220, loss 5.194673538208008, train r2 0.9391070104010414, test_r2 0.7889913016391668\n",
            "epoch 1230, loss 5.1829376220703125, train r2 0.9392445828219915, test_r2 0.789174268190344\n",
            "epoch 1240, loss 5.1722564697265625, train r2 0.9393697863078228, test_r2 0.789287878589158\n",
            "epoch 1250, loss 5.161435604095459, train r2 0.9394966325104332, test_r2 0.789093288503727\n",
            "epoch 1260, loss 5.149646282196045, train r2 0.9396348309061671, test_r2 0.7894507212559005\n",
            "epoch 1270, loss 5.136282444000244, train r2 0.9397914796495214, test_r2 0.7893694556373191\n",
            "epoch 1280, loss 5.122707843780518, train r2 0.9399506083065076, test_r2 0.7892650367258693\n",
            "epoch 1290, loss 5.110459327697754, train r2 0.9400941865125705, test_r2 0.7896657879779196\n",
            "epoch 1300, loss 5.098302841186523, train r2 0.9402366874094555, test_r2 0.7892399673585382\n",
            "epoch 1310, loss 5.084573745727539, train r2 0.940397623079228, test_r2 0.789830955457913\n",
            "epoch 1320, loss 5.072274684906006, train r2 0.9405417937068979, test_r2 0.7901434246118335\n",
            "epoch 1330, loss 5.060236930847168, train r2 0.9406829038650182, test_r2 0.7902413095886275\n",
            "epoch 1340, loss 5.046414375305176, train r2 0.9408449331498712, test_r2 0.7909012149450105\n",
            "epoch 1350, loss 5.033172607421875, train r2 0.9410001544077299, test_r2 0.7913037711719784\n",
            "epoch 1360, loss 5.020658493041992, train r2 0.9411468499527011, test_r2 0.7914440229423328\n",
            "epoch 1370, loss 5.0099778175354, train r2 0.9412720520185636, test_r2 0.7912254730170273\n",
            "epoch 1380, loss 5.001970291137695, train r2 0.9413659168594211, test_r2 0.7913916928707987\n",
            "epoch 1390, loss 4.993587493896484, train r2 0.9414641817028824, test_r2 0.7915104390098824\n",
            "epoch 1400, loss 4.985518455505371, train r2 0.9415587683121417, test_r2 0.7914706201435096\n",
            "epoch 1410, loss 4.975905418395996, train r2 0.9416714505210175, test_r2 0.7919943309940065\n",
            "epoch 1420, loss 4.9676194190979, train r2 0.9417685821246089, test_r2 0.7924911943212979\n",
            "epoch 1430, loss 4.958256721496582, train r2 0.9418783353161354, test_r2 0.7927342776228637\n",
            "epoch 1440, loss 4.947670936584473, train r2 0.9420024232613251, test_r2 0.7928235107563824\n",
            "epoch 1450, loss 4.937788486480713, train r2 0.9421182671261447, test_r2 0.7930948573708322\n",
            "epoch 1460, loss 4.928683280944824, train r2 0.9422250002500286, test_r2 0.7936157867700491\n",
            "epoch 1470, loss 4.919427871704102, train r2 0.9423334939829415, test_r2 0.7939569646654459\n",
            "epoch 1480, loss 4.910129547119141, train r2 0.9424424917110812, test_r2 0.7939658203986104\n",
            "epoch 1490, loss 4.900811195373535, train r2 0.9425517234608293, test_r2 0.794186058167025\n",
            "epoch 1500, loss 4.892332077026367, train r2 0.9426511186532676, test_r2 0.7940366617769141\n",
            "epoch 1510, loss 4.883436679840088, train r2 0.9427553903961167, test_r2 0.7941216373637133\n",
            "epoch 1520, loss 4.874209403991699, train r2 0.9428635565684356, test_r2 0.7944125492962676\n",
            "epoch 1530, loss 4.866479396820068, train r2 0.9429541682417582, test_r2 0.7947934096416025\n",
            "epoch 1540, loss 4.8578314781188965, train r2 0.9430555413267715, test_r2 0.7946481080072558\n",
            "epoch 1550, loss 4.847543239593506, train r2 0.9431761419065688, test_r2 0.7946872746226729\n",
            "epoch 1560, loss 4.836663246154785, train r2 0.9433036774668635, test_r2 0.7948901271086641\n",
            "epoch 1570, loss 4.823843479156494, train r2 0.9434539553029531, test_r2 0.79552789011646\n",
            "epoch 1580, loss 4.810840129852295, train r2 0.9436063813514244, test_r2 0.7954784091971572\n",
            "epoch 1590, loss 4.797808647155762, train r2 0.9437591394684806, test_r2 0.7958879949438115\n",
            "epoch 1600, loss 4.783365726470947, train r2 0.9439284438909915, test_r2 0.7963683388809208\n",
            "epoch 1610, loss 4.770983695983887, train r2 0.9440735880030549, test_r2 0.7964018638821104\n",
            "epoch 1620, loss 4.759815216064453, train r2 0.9442045072626566, test_r2 0.7970019839874833\n",
            "epoch 1630, loss 4.748996257781982, train r2 0.9443313288717683, test_r2 0.7970544077949393\n",
            "epoch 1640, loss 4.737159729003906, train r2 0.9444700791269162, test_r2 0.7968758847499356\n",
            "epoch 1650, loss 4.723283767700195, train r2 0.9446327321134473, test_r2 0.7973512130983821\n",
            "epoch 1660, loss 4.711633205413818, train r2 0.9447693058511074, test_r2 0.7974045437406998\n",
            "epoch 1670, loss 4.69889497756958, train r2 0.9449186218668568, test_r2 0.7976039960650176\n",
            "epoch 1680, loss 4.688265800476074, train r2 0.9450432221036017, test_r2 0.7973826441924408\n",
            "epoch 1690, loss 4.676979064941406, train r2 0.9451755293048897, test_r2 0.7973876080833724\n",
            "epoch 1700, loss 4.666769027709961, train r2 0.9452952103257607, test_r2 0.7977395651640995\n",
            "epoch 1710, loss 4.656558513641357, train r2 0.9454149015040224, test_r2 0.7978451926217361\n",
            "epoch 1720, loss 4.64695930480957, train r2 0.9455274258696005, test_r2 0.7977825952688119\n",
            "epoch 1730, loss 4.638632774353027, train r2 0.9456250292066752, test_r2 0.7977923819181832\n",
            "epoch 1740, loss 4.629303455352783, train r2 0.9457343912935383, test_r2 0.7978099690481282\n",
            "epoch 1750, loss 4.621952533721924, train r2 0.945820557386041, test_r2 0.7975257698681063\n",
            "epoch 1760, loss 4.613719940185547, train r2 0.9459170629223277, test_r2 0.7981503131871108\n",
            "epoch 1770, loss 4.606492519378662, train r2 0.946001785948163, test_r2 0.797543969883156\n",
            "epoch 1780, loss 4.598474502563477, train r2 0.9460957708112991, test_r2 0.7980857560106073\n",
            "epoch 1790, loss 4.589373588562012, train r2 0.9462024580773261, test_r2 0.797709397746865\n",
            "epoch 1800, loss 4.580958366394043, train r2 0.9463011003049289, test_r2 0.7981237099527276\n",
            "epoch 1810, loss 4.572774887084961, train r2 0.9463970274652752, test_r2 0.797916619987167\n",
            "epoch 1820, loss 4.566183567047119, train r2 0.946474292801546, test_r2 0.7978275383501414\n",
            "epoch 1830, loss 4.559564590454102, train r2 0.9465518823184963, test_r2 0.7976812366155244\n",
            "epoch 1840, loss 4.552983283996582, train r2 0.9466290293367992, test_r2 0.7978376672418944\n",
            "epoch 1850, loss 4.547688961029053, train r2 0.9466910915039942, test_r2 0.7979156560619587\n",
            "epoch 1860, loss 4.542050838470459, train r2 0.9467571805994569, test_r2 0.7975042831901168\n",
            "epoch 1870, loss 4.536851406097412, train r2 0.9468181310376333, test_r2 0.797844959268406\n",
            "epoch 1880, loss 4.5308051109313965, train r2 0.9468890081297802, test_r2 0.797702414080319\n",
            "epoch 1890, loss 4.525429725646973, train r2 0.9469520156712932, test_r2 0.7975550292743125\n",
            "epoch 1900, loss 4.521063327789307, train r2 0.9470032039337097, test_r2 0.7975126186369557\n",
            "epoch 1910, loss 4.515743732452393, train r2 0.9470655569289763, test_r2 0.7974169964965905\n",
            "epoch 1920, loss 4.5109357833862305, train r2 0.9471219207654542, test_r2 0.7972929631576159\n",
            "epoch 1930, loss 4.507008075714111, train r2 0.9471679601958121, test_r2 0.7975179642933713\n",
            "epoch 1940, loss 4.502467155456543, train r2 0.9472211911583369, test_r2 0.796902753858253\n",
            "epoch 1950, loss 4.4980854988098145, train r2 0.947272553772448, test_r2 0.7969881672904279\n",
            "epoch 1960, loss 4.493580341339111, train r2 0.9473253632490198, test_r2 0.7968555615357831\n",
            "epoch 1970, loss 4.490253925323486, train r2 0.9473643540307624, test_r2 0.7966831163607426\n",
            "epoch 1980, loss 4.486975193023682, train r2 0.9474027915189129, test_r2 0.7971263880543294\n",
            "epoch 1990, loss 4.482715606689453, train r2 0.9474527205539305, test_r2 0.7971253159246525\n",
            "epoch 2000, loss 4.478550910949707, train r2 0.9475015421376004, test_r2 0.7971992046957462\n",
            "epoch 2010, loss 4.470616817474365, train r2 0.9475945472711096, test_r2 0.7971273182374488\n",
            "epoch 2020, loss 4.460987567901611, train r2 0.947707422374232, test_r2 0.7970070998228069\n",
            "epoch 2030, loss 4.453348636627197, train r2 0.9477969683557002, test_r2 0.7972041797909606\n",
            "epoch 2040, loss 4.448328495025635, train r2 0.9478558135640427, test_r2 0.7969554129826265\n",
            "epoch 2050, loss 4.444124698638916, train r2 0.9479050937637876, test_r2 0.7969615564923874\n",
            "epoch 2060, loss 4.43902587890625, train r2 0.9479648610167977, test_r2 0.7968928298981205\n",
            "epoch 2070, loss 4.434617519378662, train r2 0.948016538651479, test_r2 0.7971075710605657\n",
            "epoch 2080, loss 4.430164337158203, train r2 0.9480687367398662, test_r2 0.7969771813196473\n",
            "epoch 2090, loss 4.427033424377441, train r2 0.9481054406970029, test_r2 0.7968179973856004\n",
            "epoch 2100, loss 4.4224138259887695, train r2 0.9481595914157293, test_r2 0.7967225704844889\n",
            "epoch 2110, loss 4.4192681312561035, train r2 0.9481964626657423, test_r2 0.7966822918681303\n",
            "epoch 2120, loss 4.4161696434021, train r2 0.9482327870318082, test_r2 0.7968987626941855\n",
            "epoch 2130, loss 4.41239070892334, train r2 0.9482770859839558, test_r2 0.7965117659515133\n",
            "epoch 2140, loss 4.407496929168701, train r2 0.9483344491164653, test_r2 0.7967496452982844\n",
            "epoch 2150, loss 4.403774738311768, train r2 0.9483780792603543, test_r2 0.7968742437475691\n",
            "epoch 2160, loss 4.400464057922363, train r2 0.9484168886759254, test_r2 0.7969852497039444\n",
            "epoch 2170, loss 4.396646976470947, train r2 0.9484616322583164, test_r2 0.7968908877473072\n",
            "epoch 2180, loss 4.393548965454102, train r2 0.948497949245838, test_r2 0.7971736436407962\n",
            "epoch 2190, loss 4.389590263366699, train r2 0.948544354428269, test_r2 0.7969136745470449\n",
            "epoch 2200, loss 4.385763645172119, train r2 0.9485892130660375, test_r2 0.7969061543258098\n",
            "epoch 2210, loss 4.382323265075684, train r2 0.9486295390799213, test_r2 0.7969235451098016\n",
            "epoch 2220, loss 4.379967212677002, train r2 0.9486571598008792, test_r2 0.7967486199217552\n",
            "epoch 2230, loss 4.376491069793701, train r2 0.9486979089418859, test_r2 0.7967284883515178\n",
            "epoch 2240, loss 4.3724870681762695, train r2 0.9487448432058359, test_r2 0.7970190972171682\n",
            "epoch 2250, loss 4.369337558746338, train r2 0.9487817634816696, test_r2 0.7970791410392588\n",
            "epoch 2260, loss 4.365851402282715, train r2 0.9488226279114532, test_r2 0.7973070978789022\n",
            "epoch 2270, loss 4.3616437911987305, train r2 0.9488719484881862, test_r2 0.7971951677746761\n",
            "epoch 2280, loss 4.358377456665039, train r2 0.9489102352676154, test_r2 0.7970656444855593\n",
            "epoch 2290, loss 4.354535102844238, train r2 0.9489552810891757, test_r2 0.797034708300853\n",
            "epoch 2300, loss 4.350575923919678, train r2 0.9490016898034326, test_r2 0.7974011045408469\n",
            "epoch 2310, loss 4.3473896980285645, train r2 0.9490390392179221, test_r2 0.7975232863330493\n",
            "epoch 2320, loss 4.343343257904053, train r2 0.9490864700633513, test_r2 0.7971288897547051\n",
            "epoch 2330, loss 4.3408122062683105, train r2 0.9491161427958444, test_r2 0.7973653667497165\n",
            "epoch 2340, loss 4.337021827697754, train r2 0.9491605715269414, test_r2 0.7972560072150695\n",
            "epoch 2350, loss 4.33358097076416, train r2 0.949200907125669, test_r2 0.7972030328333202\n",
            "epoch 2360, loss 4.330080509185791, train r2 0.9492419380865097, test_r2 0.7970577438941061\n",
            "epoch 2370, loss 4.3263139724731445, train r2 0.9492860932970584, test_r2 0.7971430256662217\n",
            "epoch 2380, loss 4.323456764221191, train r2 0.9493195837864087, test_r2 0.7973223711713464\n",
            "epoch 2390, loss 4.320922374725342, train r2 0.9493492933478128, test_r2 0.7975865743174825\n",
            "epoch 2400, loss 4.317256927490234, train r2 0.9493922634392101, test_r2 0.7972775978629841\n",
            "epoch 2410, loss 4.312713623046875, train r2 0.9494455197071027, test_r2 0.7969785260036055\n",
            "epoch 2420, loss 4.309710502624512, train r2 0.949480722674561, test_r2 0.7970521395303254\n",
            "epoch 2430, loss 4.3064422607421875, train r2 0.949519032292894, test_r2 0.7970660632030173\n",
            "epoch 2440, loss 4.304011344909668, train r2 0.949547526254071, test_r2 0.7967153210936773\n",
            "epoch 2450, loss 4.3002166748046875, train r2 0.9495920103839687, test_r2 0.796996219809652\n",
            "epoch 2460, loss 4.2969136238098145, train r2 0.9496307285988688, test_r2 0.7970022705224213\n",
            "epoch 2470, loss 4.2933220863342285, train r2 0.9496728285721453, test_r2 0.7967535812959294\n",
            "epoch 2480, loss 4.290163040161133, train r2 0.9497098594077267, test_r2 0.7970986595314046\n",
            "epoch 2490, loss 4.287513732910156, train r2 0.9497409179574607, test_r2 0.797152954402344\n",
            "epoch 2500, loss 4.284006595611572, train r2 0.9497820273028564, test_r2 0.7966345567321415\n",
            "epoch 2510, loss 4.2795491218566895, train r2 0.9498342765833273, test_r2 0.7967888323804229\n",
            "epoch 2520, loss 4.276092529296875, train r2 0.9498747987045252, test_r2 0.7968156309743211\n",
            "epoch 2530, loss 4.272874355316162, train r2 0.9499125207671096, test_r2 0.7969061718298891\n",
            "epoch 2540, loss 4.26976203918457, train r2 0.9499490070246276, test_r2 0.7967041532582765\n",
            "epoch 2550, loss 4.2664995193481445, train r2 0.9499872481407652, test_r2 0.7967469227254265\n",
            "epoch 2560, loss 4.2623796463012695, train r2 0.9500355451661069, test_r2 0.7965820865412389\n",
            "epoch 2570, loss 4.257835865020752, train r2 0.9500888076140385, test_r2 0.7966303174293552\n",
            "epoch 2580, loss 4.255248069763184, train r2 0.9501191393744856, test_r2 0.7965788940877943\n",
            "epoch 2590, loss 4.252389430999756, train r2 0.9501526476180716, test_r2 0.7966036537256755\n",
            "epoch 2600, loss 4.248427867889404, train r2 0.9501990879466352, test_r2 0.7964930391035945\n",
            "epoch 2610, loss 4.243748664855957, train r2 0.9502539393812717, test_r2 0.7964159674183349\n",
            "epoch 2620, loss 4.240814208984375, train r2 0.9502883375894884, test_r2 0.7965662840461541\n",
            "epoch 2630, loss 4.237457275390625, train r2 0.9503276869656937, test_r2 0.7964096403937625\n",
            "epoch 2640, loss 4.234142303466797, train r2 0.9503665489186204, test_r2 0.7961183669473275\n",
            "epoch 2650, loss 4.230930805206299, train r2 0.9504041906449521, test_r2 0.796390564066312\n",
            "epoch 2660, loss 4.2276716232299805, train r2 0.9504423956252551, test_r2 0.796422737988267\n",
            "epoch 2670, loss 4.225447654724121, train r2 0.9504684660399749, test_r2 0.796327026325174\n",
            "epoch 2680, loss 4.222066879272461, train r2 0.950508097848806, test_r2 0.7964174160243243\n",
            "epoch 2690, loss 4.21860408782959, train r2 0.9505486892094526, test_r2 0.7964366649854686\n",
            "epoch 2700, loss 4.21531343460083, train r2 0.950587263053329, test_r2 0.7962233841040132\n",
            "epoch 2710, loss 4.210705757141113, train r2 0.9506412773134287, test_r2 0.7960688641865723\n",
            "epoch 2720, loss 4.207139015197754, train r2 0.9506830852027425, test_r2 0.795901303277507\n",
            "epoch 2730, loss 4.203925132751465, train r2 0.9507207588380067, test_r2 0.795844705972069\n",
            "epoch 2740, loss 4.200525760650635, train r2 0.9507606038638718, test_r2 0.7961351570799354\n",
            "epoch 2750, loss 4.1966376304626465, train r2 0.950806183180481, test_r2 0.7958812256826615\n",
            "epoch 2760, loss 4.19249963760376, train r2 0.950854690912855, test_r2 0.7958455224485436\n",
            "epoch 2770, loss 4.189641952514648, train r2 0.9508881869962802, test_r2 0.7957663657228841\n",
            "epoch 2780, loss 4.184704303741455, train r2 0.9509460696804721, test_r2 0.7957685512215974\n",
            "epoch 2790, loss 4.1786932945251465, train r2 0.9510165282430847, test_r2 0.7956257629513723\n",
            "epoch 2800, loss 4.173691272735596, train r2 0.951075163882643, test_r2 0.7957165958871392\n",
            "epoch 2810, loss 4.167788982391357, train r2 0.951144354803185, test_r2 0.795698902864088\n",
            "epoch 2820, loss 4.161828517913818, train r2 0.9512142237980418, test_r2 0.7955584623909933\n",
            "epoch 2830, loss 4.156554222106934, train r2 0.9512760473777836, test_r2 0.7955121067774648\n",
            "epoch 2840, loss 4.151053428649902, train r2 0.9513405301819992, test_r2 0.7956821016435628\n",
            "epoch 2850, loss 4.146900177001953, train r2 0.9513892188304763, test_r2 0.7957340027787972\n",
            "epoch 2860, loss 4.143760681152344, train r2 0.9514260187012948, test_r2 0.795610877607472\n",
            "epoch 2870, loss 4.139317989349365, train r2 0.9514780951058115, test_r2 0.7952758166112661\n",
            "epoch 2880, loss 4.135965824127197, train r2 0.9515173926026844, test_r2 0.7951714877545795\n",
            "epoch 2890, loss 4.131454944610596, train r2 0.9515702698070193, test_r2 0.795386399241538\n",
            "epoch 2900, loss 4.127683639526367, train r2 0.9516144745800923, test_r2 0.7951907036548326\n",
            "epoch 2910, loss 4.123188495635986, train r2 0.9516671694846363, test_r2 0.7955087678190162\n",
            "epoch 2920, loss 4.117549419403076, train r2 0.9517332739126195, test_r2 0.7958241155466131\n",
            "epoch 2930, loss 4.1120805740356445, train r2 0.9517973762237258, test_r2 0.7959334066906034\n",
            "epoch 2940, loss 4.106710433959961, train r2 0.9518603259322881, test_r2 0.7956314938385419\n",
            "epoch 2950, loss 4.102011203765869, train r2 0.9519154140309735, test_r2 0.7960135229178174\n",
            "epoch 2960, loss 4.0957441329956055, train r2 0.9519888764852701, test_r2 0.7956675407568007\n",
            "epoch 2970, loss 4.09110689163208, train r2 0.952043237118142, test_r2 0.7959704471315266\n",
            "epoch 2980, loss 4.087091445922852, train r2 0.9520903060578836, test_r2 0.7959306531525006\n",
            "epoch 2990, loss 4.083583831787109, train r2 0.9521314210976447, test_r2 0.7957324278229808\n",
            "epoch 3000, loss 4.079218864440918, train r2 0.9521825876020248, test_r2 0.7960530415710938\n",
            "epoch 3010, loss 4.075056076049805, train r2 0.9522313893756228, test_r2 0.7960304993851843\n",
            "epoch 3020, loss 4.069980144500732, train r2 0.9522908855476822, test_r2 0.7959102243564911\n",
            "epoch 3030, loss 4.066070556640625, train r2 0.9523367175846008, test_r2 0.7957974548019084\n",
            "epoch 3040, loss 4.062864780426025, train r2 0.9523742943865731, test_r2 0.7957018506267908\n",
            "epoch 3050, loss 4.058253288269043, train r2 0.9524283528606035, test_r2 0.7955021221965681\n",
            "epoch 3060, loss 4.053776264190674, train r2 0.9524808330613315, test_r2 0.7956569830171699\n",
            "epoch 3070, loss 4.049942970275879, train r2 0.952525769739543, test_r2 0.7954867356373602\n",
            "epoch 3080, loss 4.04640531539917, train r2 0.9525672366580694, test_r2 0.7955332982905705\n",
            "epoch 3090, loss 4.042360305786133, train r2 0.9526146552834452, test_r2 0.7953863656112254\n",
            "epoch 3100, loss 4.037519931793213, train r2 0.9526713903785353, test_r2 0.7952167890555304\n",
            "epoch 3110, loss 4.035439491271973, train r2 0.9526957830502435, test_r2 0.7951148169706365\n",
            "epoch 3120, loss 4.029325008392334, train r2 0.9527674568326261, test_r2 0.795016395898668\n",
            "epoch 3130, loss 4.0248918533325195, train r2 0.9528194207885649, test_r2 0.7949765718750212\n",
            "epoch 3140, loss 4.021219253540039, train r2 0.9528624712571626, test_r2 0.7952489173463171\n",
            "epoch 3150, loss 4.01482629776001, train r2 0.9529374106186302, test_r2 0.7952098643742629\n",
            "epoch 3160, loss 4.007645606994629, train r2 0.9530215861022142, test_r2 0.7948339580420933\n",
            "epoch 3170, loss 4.000667572021484, train r2 0.9531033835989328, test_r2 0.7950762063355034\n",
            "epoch 3180, loss 3.9960365295410156, train r2 0.9531576699256779, test_r2 0.794805743465403\n",
            "epoch 3190, loss 3.989811420440674, train r2 0.9532306411114199, test_r2 0.7950961342643758\n",
            "epoch 3200, loss 3.984236478805542, train r2 0.9532959914228892, test_r2 0.7949942701117703\n",
            "epoch 3210, loss 3.9794681072235107, train r2 0.9533518866313604, test_r2 0.7950317179435376\n",
            "epoch 3220, loss 3.9746599197387695, train r2 0.9534082499679951, test_r2 0.7951417500109089\n",
            "epoch 3230, loss 3.9699513912200928, train r2 0.9534634447443774, test_r2 0.7952452805754567\n",
            "epoch 3240, loss 3.964686870574951, train r2 0.9535251566777302, test_r2 0.7950559868363988\n",
            "epoch 3250, loss 3.960700273513794, train r2 0.9535718870245695, test_r2 0.7943491486451474\n",
            "epoch 3260, loss 3.95619535446167, train r2 0.953624696920943, test_r2 0.794731261573919\n",
            "epoch 3270, loss 3.9515957832336426, train r2 0.953678612472876, test_r2 0.7945336021009411\n",
            "epoch 3280, loss 3.9472899436950684, train r2 0.9537290856057581, test_r2 0.7942618077620789\n",
            "epoch 3290, loss 3.943984031677246, train r2 0.95376783783844, test_r2 0.7939755357816416\n",
            "epoch 3300, loss 3.940000295639038, train r2 0.9538145378773222, test_r2 0.7939767531102828\n",
            "epoch 3310, loss 3.9368860721588135, train r2 0.9538510427300073, test_r2 0.7943395051209821\n",
            "epoch 3320, loss 3.933260917663574, train r2 0.9538935367634086, test_r2 0.7941591462267068\n",
            "epoch 3330, loss 3.9304757118225098, train r2 0.9539261860387628, test_r2 0.7940334005641458\n",
            "epoch 3340, loss 3.9284844398498535, train r2 0.9539495274785222, test_r2 0.7942187297439134\n",
            "epoch 3350, loss 3.9256699085235596, train r2 0.953982521255548, test_r2 0.7940099456580634\n",
            "epoch 3360, loss 3.922496795654297, train r2 0.9540197162179521, test_r2 0.7940620313237214\n",
            "epoch 3370, loss 3.9204416275024414, train r2 0.9540438074073148, test_r2 0.7939969503307314\n",
            "epoch 3380, loss 3.9168214797973633, train r2 0.954086244989132, test_r2 0.7938551916776435\n",
            "epoch 3390, loss 3.9147398471832275, train r2 0.9541106446333976, test_r2 0.7937999501526352\n",
            "epoch 3400, loss 3.9120209217071533, train r2 0.9541425182041281, test_r2 0.7941257566658199\n",
            "epoch 3410, loss 3.910670518875122, train r2 0.954158347705363, test_r2 0.7942611583800998\n",
            "epoch 3420, loss 3.9073903560638428, train r2 0.9541967979128496, test_r2 0.7935959339333504\n",
            "epoch 3430, loss 3.9056687355041504, train r2 0.9542169781925328, test_r2 0.7935459914002257\n",
            "epoch 3440, loss 3.9032225608825684, train r2 0.9542456538795313, test_r2 0.7934009257953488\n",
            "epoch 3450, loss 3.900639772415161, train r2 0.9542759299202894, test_r2 0.7940433581590609\n",
            "epoch 3460, loss 3.8989157676696777, train r2 0.9542961391993522, test_r2 0.7934958516751099\n",
            "epoch 3470, loss 3.8961942195892334, train r2 0.9543280413207023, test_r2 0.793218634569285\n",
            "epoch 3480, loss 3.8938796520233154, train r2 0.9543551712981773, test_r2 0.7938262437085982\n",
            "epoch 3490, loss 3.8914108276367188, train r2 0.9543841123566871, test_r2 0.7935238651778292\n",
            "epoch 3500, loss 3.88913631439209, train r2 0.9544107749910092, test_r2 0.7937156156179238\n",
            "epoch 3510, loss 3.8869645595550537, train r2 0.9544362333916137, test_r2 0.7935381802054751\n",
            "epoch 3520, loss 3.884843349456787, train r2 0.9544610991527365, test_r2 0.7934609000388293\n",
            "epoch 3530, loss 3.8824219703674316, train r2 0.954489483078025, test_r2 0.7934750904535189\n",
            "epoch 3540, loss 3.88008451461792, train r2 0.9545168818425297, test_r2 0.7932360543647803\n",
            "epoch 3550, loss 3.8775856494903564, train r2 0.9545461743642918, test_r2 0.7931380097839296\n",
            "epoch 3560, loss 3.8760809898376465, train r2 0.9545638110244977, test_r2 0.793114323799099\n",
            "epoch 3570, loss 3.8742284774780273, train r2 0.9545855279267773, test_r2 0.7929940533635974\n",
            "epoch 3580, loss 3.87210750579834, train r2 0.9546103903319744, test_r2 0.7930169045830547\n",
            "epoch 3590, loss 3.869518280029297, train r2 0.9546407401796219, test_r2 0.7925694841175456\n",
            "epoch 3600, loss 3.8677709102630615, train r2 0.9546612246136909, test_r2 0.7930760104237653\n",
            "epoch 3610, loss 3.8656601905822754, train r2 0.954685966881176, test_r2 0.7930205305446174\n",
            "epoch 3620, loss 3.8639590740203857, train r2 0.9547059070158344, test_r2 0.7927931363958842\n",
            "epoch 3630, loss 3.862208366394043, train r2 0.9547264303437774, test_r2 0.7925586969163376\n",
            "epoch 3640, loss 3.860459804534912, train r2 0.9547469269979828, test_r2 0.7922422074985592\n",
            "epoch 3650, loss 3.8579390048980713, train r2 0.9547764755346021, test_r2 0.7922723676592337\n",
            "epoch 3660, loss 3.8558506965637207, train r2 0.9548009550893943, test_r2 0.7923731686710542\n",
            "epoch 3670, loss 3.854231834411621, train r2 0.9548199327633471, test_r2 0.7919421711728337\n",
            "epoch 3680, loss 3.8518970012664795, train r2 0.9548473029164393, test_r2 0.7922619061075941\n",
            "epoch 3690, loss 3.850492000579834, train r2 0.9548637721997992, test_r2 0.7916036971491607\n",
            "epoch 3700, loss 3.848123073577881, train r2 0.9548915401370618, test_r2 0.7921018913972715\n",
            "epoch 3710, loss 3.845642328262329, train r2 0.9549206210935761, test_r2 0.7917522385321285\n",
            "epoch 3720, loss 3.843188762664795, train r2 0.9549493816316857, test_r2 0.7914881379931904\n",
            "epoch 3730, loss 3.8396694660186768, train r2 0.9549906337492635, test_r2 0.7914725806401879\n",
            "epoch 3740, loss 3.8375844955444336, train r2 0.9550150746548032, test_r2 0.7911525512905535\n",
            "epoch 3750, loss 3.8348841667175293, train r2 0.955046730235379, test_r2 0.7911363524163811\n",
            "epoch 3760, loss 3.8319826126098633, train r2 0.9550807427409705, test_r2 0.7909653140753787\n",
            "epoch 3770, loss 3.82769775390625, train r2 0.9551309703584647, test_r2 0.7909435401928683\n",
            "epoch 3780, loss 3.8239548206329346, train r2 0.9551748459441931, test_r2 0.7905741494479258\n",
            "epoch 3790, loss 3.8188624382019043, train r2 0.9552345389793521, test_r2 0.7906580189203863\n",
            "epoch 3800, loss 3.8137874603271484, train r2 0.9552940291290679, test_r2 0.7902995221089193\n",
            "epoch 3810, loss 3.809831142425537, train r2 0.9553404067041426, test_r2 0.7900478674107774\n",
            "epoch 3820, loss 3.8051095008850098, train r2 0.9553957540412487, test_r2 0.7896241234000805\n",
            "epoch 3830, loss 3.800682306289673, train r2 0.9554476504958952, test_r2 0.7896195225799509\n",
            "epoch 3840, loss 3.796734571456909, train r2 0.9554939274746593, test_r2 0.7897558620279035\n",
            "epoch 3850, loss 3.7927491664886475, train r2 0.9555406450530795, test_r2 0.7896444084136054\n",
            "epoch 3860, loss 3.7890162467956543, train r2 0.9555844025441249, test_r2 0.7895765308525705\n",
            "epoch 3870, loss 3.785684108734131, train r2 0.9556234625141713, test_r2 0.789136192494554\n",
            "epoch 3880, loss 3.7821412086486816, train r2 0.9556649917372625, test_r2 0.7894830270650847\n",
            "epoch 3890, loss 3.779043436050415, train r2 0.9557013043176403, test_r2 0.790187535465013\n",
            "epoch 3900, loss 3.774968385696411, train r2 0.9557490748060277, test_r2 0.7899993247716632\n",
            "epoch 3910, loss 3.7699174880981445, train r2 0.9558082817213635, test_r2 0.7894466532662588\n",
            "epoch 3920, loss 3.7660083770751953, train r2 0.9558541050718334, test_r2 0.7897660316405676\n",
            "epoch 3930, loss 3.761875629425049, train r2 0.9559025510057263, test_r2 0.7896454446672925\n",
            "epoch 3940, loss 3.7580697536468506, train r2 0.9559471643713022, test_r2 0.7894670460543232\n",
            "epoch 3950, loss 3.7536730766296387, train r2 0.9559987032751137, test_r2 0.7895461846023965\n",
            "epoch 3960, loss 3.7499029636383057, train r2 0.9560428946616929, test_r2 0.7897788293047345\n",
            "epoch 3970, loss 3.7473323345184326, train r2 0.95607302918571, test_r2 0.7901002375886853\n",
            "epoch 3980, loss 3.7388594150543213, train r2 0.9561723491957816, test_r2 0.7896904805706741\n",
            "epoch 3990, loss 3.733593463897705, train r2 0.95623407935848, test_r2 0.7893351882353911\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5qFmksN6PCZ",
        "colab_type": "code",
        "outputId": "5aecca49-a094-4a48-ee68-15fd548b2635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.plot(loss_item)\n",
        "plt.plot(loss_test)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f4a1711e550>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6PcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_59Xa6Sw3OrE",
        "colab_type": "text"
      },
      "source": [
        "4. test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0ucHLWG3NxM",
        "colab_type": "code",
        "outputId": "b9b61593-cad5-41bf-c3b4-b7bdd6e641d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    test_input = Variable(torch.from_numpy(x_test_scaled).cuda())\n",
        "    test_label = Variable(torch.from_numpy(y_test).cuda())\n",
        "else:\n",
        "    test_input = Variable(torch.from_numpy(x_test_scaled))\n",
        "    test_label = Variable(torch.from_numpy(y_test))\n",
        "y_pred = model(test_input.float())\n",
        "test_loss = criterion(y_pred, test_label.float())\n",
        "\n",
        "print(\"prediction loss : \",test_loss)\n",
        "print(\"r2 score : \",r2_score(y_test,y_pred.detach().numpy()))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction loss :  tensor(158.3354, grad_fn=<MseLossBackward>)\n",
            "r2 score :  0.7888014412930328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([127])) that is different to the input size (torch.Size([127, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MfjxW2H9XlH",
        "colab_type": "code",
        "outputId": "599a17ef-f354-4dce-b98a-fb3395d57a44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print(\"answer : \",y_test[0:10])\n",
        "print(\"prediction : \",y_pred[0:10].T)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "answer :  [22.6 50.  23.   8.3 21.2 19.9 20.6 18.7 16.1 18.6]\n",
            "prediction :  tensor([[24.0522, 26.3348, 24.6311, 11.7977, 20.6166, 19.9739, 22.7651, 22.1031,\n",
            "         17.2671, 15.9400]], grad_fn=<PermuteBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chn9iAY7viJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}